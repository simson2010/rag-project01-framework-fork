{
  "query": "什么是DeepSeek的训练原理，用100字说明。",
  "timestamp": "2025-03-13T21:25:13.256546",
  "provider": "deepseek",
  "model": "deepseek-v3",
  "response": "DeepSeek的训练原理主要基于大规模强化学习（RL），直接在基础模型上应用RL，而不依赖监督微调（SFT）作为初步步骤。这种方法使模型能够通过自我验证、反思和生成长思维链来解决复杂问题。训练过程包括两个RL阶段和两个SFT阶段，旨在发现改进的推理模式并与人类偏好对齐。此外，通过蒸馏技术，将大模型的推理能力提炼到小模型中，提升其性能。",
  "context": [
    {
      "text": "1.1. 贡献\n训练后：在基础模型上进行大规模强化学习\n• 我们直接将强化学习（ RL ）应用于基础模型，而不依赖于监督微调（ SFT ）作为初步步骤\n。这种方法使模型能够探索思维链（ CoT ）以解决复杂问题，从而开发出 DeepSeek-R1-Zer\no 。DeepSeek-R1-Zero 展示了自我验证、反思和生成长思维链等能力，标志着研究社区的一\n个重要里程碑。值得注意的是，这是首个公开的研究，验证了 LLMs 的推理能力可以纯粹\n通过RL 激励，而不需要 SFT 。这一突破为这一领域的未来进展铺平了道路。\n• 我们介绍了开发 DeepSeek-R1 的流程。该流程包含两个 RL 阶段，旨在发现改进的推理模式\n并与人类偏好对齐，以及两个 SFT 阶段，作为模型推理和非推理能力的基础。我们相信该\n流程将通过创建更好的模型为行业带来益处。\n蒸馏：小型模型也能强大\n• 我们证明了较大模型的推理模式可以被提炼到较小的模型中，与通过强化学习在小模型上\n发现的推理模式相比，性能更优。开源的 DeepSeek-R1 及其API 将有益于研究社区在未来提\n炼出更好的小型模型。\n• 利用DeepSeek-R1 生成的推理数据，我们对研究社区中广泛使用的多个密集模型进行了微\n调。评估结果表明，经过蒸馏的较小密集模型在基准测试中表现尤为出色。 DeepSeek-R1-\nDistill-Qwen-7B 在AIME 2024 上达到了 55.5% ，超越了 QwQ-32B-Preview 。此外， DeepSeek-\nR1-Distill-Qwen-32B 在AIME 2024 上得分为 72.6% ，在MATH-500 上得分为 94.3% ，在LiveC\nodeBench 上得分为 57.2% 。这些结果显著优于之前的开源模型，并与 o1-mini 相当。我们向\n社区开源了基于 Qwen2.5 和Llama3 系列的1.5B 、7B 、8B 、14B 、32B 和70B 蒸馏检查点。\n1.2. 评估结果总结\n• 推理任务：（ 1 ）DeepSeek-R1 在 AIME 2024 上取得了  79.8% 的 Pass@1 分数，略微超过\n了 OpenAI-o1-1217 。在 MATH-500 上，它获得了令人印象深刻的  97.3% 的分数，与  Open\nAI-o1-1217 表现相当，并显著优于其他模型。（ 2 ）在与编码相关的任务中， DeepSeek-R1 \n在代码竞赛任务中展示了专家水平，它在  Codeforces 上获得了  2,029 的 Elo 评分，超过了  \n96.3% 的人类参赛者。对于工程相关任务， DeepSeek-R1 的表现略优于  DeepSeek-V3 ，这\n可能有助于开发人员在现实世界任务中取得更好的成果。\n• 知识：在 MMLU 、MMLU-Pro 和GPQA Diamond 等基准测试中， DeepSeek-R1 取得了出色的\n成绩，显著超越了 DeepSeek-V3 ，得分分别为 MMLU 90.8% 、MMLU-Pro 84.0% 和GPQA Di\namond 71.5% 。虽然在这些基准测试中其表现略低于 OpenAI-o1-1217 ，但DeepSeek-R1 超越\n了其他闭源模型，展示了其在教育任务中的竞争优势。在事实基准测试 SimpleQA 上，Dee\npSeek-R1 的表现优于 DeepSeek-V3 ，展示了其处理基于事实的查询的能力。类似趋势在 Ope\nnAI-o1 超越4o 的基准测试中也有所体现。\n4",
      "score": 0.5723658800125122,
      "metadata": {
        "source": "DeepSeek.pdf",
        "page": "4",
        "chunk": 4,
        "total_chunks": 5,
        "page_range": "4",
        "embedding_provider": "openai",
        "embedding_model": "text-embedding-3-small",
        "embedding_timestamp": "2025-03-13T17:13:00.598229"
      }
    },
    {
      "text": "目录\n1 引言 3\n1.1 贡献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4  \u00001.2 评估结果总结  . . . . . . . . . . . . . . . . \n. . . . . . . . . . . . . 4\n2 方法 5\n2.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5  \u00002.2 DeepSeek-R1-Zero ：基础模型\n上的强化学习  . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.1 强化学习算法  . . . . . . . . . . . . . . . . . . . . . . 5 2.2.2 奖励建模  . . . . . . . . . . . . . . . . . . . . . . \n. . . . . . . . . . 6 2.2.3 训练模板  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2.4 DeepSeek-R1-Z\nero 的性能、自我进化过程和顿悟时刻  6\n2.3 DeepSeek-R1 ：冷启动强化学习  . . . . . . . . . . . . . . . 9  \u00002.3.1 冷启动 . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . 9  \u00002.3.2 面向推理的强化学习  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10  \u00002.3.3 拒绝\n采样与监督微调  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10  \u00002.3.4 全场景强化学习  . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . 11  \u00002.4 蒸馏：赋予小模型推理能力  . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3 实验 11\n3.1 DeepSeek-R1 评估 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2 蒸馏模型评估  . . . . . . . . . . \n. . . . . . . . . . . . . . . . . . . . . 14\n4 讨论 14\n4.1 蒸馏与强化学习  . . . . . . . . . . . . . . . . . . . . . . . . 14 4.2 未成功的尝试  . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . 15\n5 结论、局限性与未来工作  16\nA 贡献与致谢  20\n2",
      "score": 0.5594154596328735,
      "metadata": {
        "source": "DeepSeek.pdf",
        "page": "2",
        "chunk": 2,
        "total_chunks": 5,
        "page_range": "2",
        "embedding_provider": "openai",
        "embedding_model": "text-embedding-3-small",
        "embedding_timestamp": "2025-03-13T17:13:00.598215"
      }
    },
    {
      "text": "• 其他方面： DeepSeek-R1 在多种任务中也表现出色，包括创意写作、通用问答、编辑、摘\n要等。它在  AlpacaEval 2.0 上实现了  87.6% 的长度控制胜率，在  Are-naHard 上实现了  92.3\n% 的胜率，展示了其智能处理非应试查询的强大能力。此外， DeepSeek-R1 在需要长上下\n文理解的任务上表现出色，在长上下文基准测试中显著优于  DeepSeek-V3 。\n2. 方法\n2.1. 概述\n先前的工作严重依赖大量监督数据来提升模型性能。在本研究中，我们证明了即使不使用监督\n微调（SFT ）作为冷启动，通过大规模强化学习（ RL ）也能显著提升推理能力。此外，加入少\n量冷启动数据可以进一步提升性能。在接下来的章节中，我们将介绍：（ 1 ）DeepSeek-R1-Zero\n，它直接将 RL 应用于基础模型，不使用任何 SFT 数据；（ 2 ）DeepSeek-R1 ，它从经过数千个长\n链思维（ CoT ）示例微调的检查点开始应用 RL ；（3 ）将DeepSeek-R1 的推理能力蒸馏到小型密\n集模型中。\n2.2. DeepSeek-R1-Zero ：基础模型上的强化学习\n强化学习在推理任务中展现了显著的有效性，正如我们之前的工作所证明的那样（ Shao 等，202\n4 ；Wang 等，2023 ）。然而，这些工作严重依赖于监督数据，而这些数据的收集非常耗时。在\n本节中，我们探索了大型语言模型（ LLMs ）在没有监督数据的情况下发展推理能力的潜力，重\n点关注它们通过纯强化学习过程的自我进化。我们首先简要概述了我们的强化学习算法，随后\n展示了一些令人兴奋的结果，并希望这能为社区提供有价值的见解。\n2.2.1.ReinforcementLearningAlgorithm\n群体相对策略优化  为了节省强化学习的训练成本，我们采用了群体相对策略优化（ GRPO ）（S\nhao 等，2024 ），该方法放弃了通常与策略模型大小相同的评论家模型，转而从群体分数中估计\n基线。具体来说，对于每个问题 𝑞 ，GRPO 从旧策略𝜋𝜃𝑜𝑙𝑑 中采样一组输出{𝑜1 、𝑜2 、··· 、𝑜𝐺} ，然后\n通过最大化以下目标来优化策略模型 𝜋𝜃 ：\nJ𝐺𝑅𝑃𝑂(𝜃)=E[𝑞∼𝑃(𝑄),{𝑜𝑖}𝐺\n𝑖=1∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)]\n1\n𝐺𝐺∑︁\n𝑖=1\u0012\nmin\u0012𝜋𝜃(𝑜𝑖|𝑞)\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞)𝐴𝑖,clip\u0012𝜋𝜃(𝑜𝑖|𝑞)\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞),1−𝜀,1+𝜀\u0013\n𝐴𝑖\u0013\n−𝛽D𝐾𝐿\u0000\n𝜋𝜃||𝜋𝑟𝑒𝑓\u0001\u0013\n,(1)\nD𝐾𝐿\u0000\n𝜋𝜃||𝜋𝑟𝑒𝑓\u0001=𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\n𝜋𝜃(𝑜𝑖|𝑞)−log𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\n𝜋𝜃(𝑜𝑖|𝑞)−1, (2)\n其中 𝜀 和 𝛽 是超参数， 𝐴𝑖 是优势，通过使用一组奖励  {𝑟1 、𝑟2 、... 、𝑟𝐺} 来计算，这些奖励对应\n于每个组内的输出：\n𝐴𝑖=𝑟𝑖−m𝑒𝑎𝑛({𝑟1,𝑟2,···,𝑟𝐺})\ns𝑡𝑑({𝑟1,𝑟2,···,𝑟𝐺}). (3)\n5",
      "score": 0.5416497588157654,
      "metadata": {
        "source": "DeepSeek.pdf",
        "page": "5",
        "chunk": 5,
        "total_chunks": 5,
        "page_range": "5",
        "embedding_provider": "openai",
        "embedding_model": "text-embedding-3-small",
        "embedding_timestamp": "2025-03-13T17:13:00.598234"
      }
    }
  ]
}