{
  "query": "DeepSeek R1用了哪些新技术",
  "collection_id": "DeepSeek_openai_20250313172026",
  "timestamp": "2025-03-13T17:21:04.548900",
  "results": [
    {
      "text": "1.1. 贡献\n训练后：在基础模型上进行大规模强化学习\n• 我们直接将强化学习（ RL ）应用于基础模型，而不依赖于监督微调（ SFT ）作为初步步骤\n。这种方法使模型能够探索思维链（ CoT ）以解决复杂问题，从而开发出 DeepSeek-R1-Zer\no 。DeepSeek-R1-Zero 展示了自我验证、反思和生成长思维链等能力，标志着研究社区的一\n个重要里程碑。值得注意的是，这是首个公开的研究，验证了 LLMs 的推理能力可以纯粹\n通过RL 激励，而不需要 SFT 。这一突破为这一领域的未来进展铺平了道路。\n• 我们介绍了开发 DeepSeek-R1 的流程。该流程包含两个 RL 阶段，旨在发现改进的推理模式\n并与人类偏好对齐，以及两个 SFT 阶段，作为模型推理和非推理能力的基础。我们相信该\n流程将通过创建更好的模型为行业带来益处。\n蒸馏：小型模型也能强大\n• 我们证明了较大模型的推理模式可以被提炼到较小的模型中，与通过强化学习在小模型上\n发现的推理模式相比，性能更优。开源的 DeepSeek-R1 及其API 将有益于研究社区在未来提\n炼出更好的小型模型。\n• 利用DeepSeek-R1 生成的推理数据，我们对研究社区中广泛使用的多个密集模型进行了微\n调。评估结果表明，经过蒸馏的较小密集模型在基准测试中表现尤为出色。 DeepSeek-R1-\nDistill-Qwen-7B 在AIME 2024 上达到了 55.5% ，超越了 QwQ-32B-Preview 。此外， DeepSeek-\nR1-Distill-Qwen-32B 在AIME 2024 上得分为 72.6% ，在MATH-500 上得分为 94.3% ，在LiveC\nodeBench 上得分为 57.2% 。这些结果显著优于之前的开源模型，并与 o1-mini 相当。我们向\n社区开源了基于 Qwen2.5 和Llama3 系列的1.5B 、7B 、8B 、14B 、32B 和70B 蒸馏检查点。\n1.2. 评估结果总结\n• 推理任务：（ 1 ）DeepSeek-R1 在 AIME 2024 上取得了  79.8% 的 Pass@1 分数，略微超过\n了 OpenAI-o1-1217 。在 MATH-500 上，它获得了令人印象深刻的  97.3% 的分数，与  Open\nAI-o1-1217 表现相当，并显著优于其他模型。（ 2 ）在与编码相关的任务中， DeepSeek-R1 \n在代码竞赛任务中展示了专家水平，它在  Codeforces 上获得了  2,029 的 Elo 评分，超过了  \n96.3% 的人类参赛者。对于工程相关任务， DeepSeek-R1 的表现略优于  DeepSeek-V3 ，这\n可能有助于开发人员在现实世界任务中取得更好的成果。\n• 知识：在 MMLU 、MMLU-Pro 和GPQA Diamond 等基准测试中， DeepSeek-R1 取得了出色的\n成绩，显著超越了 DeepSeek-V3 ，得分分别为 MMLU 90.8% 、MMLU-Pro 84.0% 和GPQA Di\namond 71.5% 。虽然在这些基准测试中其表现略低于 OpenAI-o1-1217 ，但DeepSeek-R1 超越\n了其他闭源模型，展示了其在教育任务中的竞争优势。在事实基准测试 SimpleQA 上，Dee\npSeek-R1 的表现优于 DeepSeek-V3 ，展示了其处理基于事实的查询的能力。类似趋势在 Ope\nnAI-o1 超越4o 的基准测试中也有所体现。\n4",
      "score": 0.582477331161499,
      "metadata": {
        "source": "DeepSeek.pdf",
        "page": "4",
        "chunk": 4,
        "total_chunks": 5,
        "page_range": "4",
        "embedding_provider": "openai",
        "embedding_model": "text-embedding-3-small",
        "embedding_timestamp": "2025-03-13T17:13:00.598229"
      }
    },
    {
      "text": "目录\n1 引言 3\n1.1 贡献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4  \u00001.2 评估结果总结  . . . . . . . . . . . . . . . . \n. . . . . . . . . . . . . 4\n2 方法 5\n2.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5  \u00002.2 DeepSeek-R1-Zero ：基础模型\n上的强化学习  . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.1 强化学习算法  . . . . . . . . . . . . . . . . . . . . . . 5 2.2.2 奖励建模  . . . . . . . . . . . . . . . . . . . . . . \n. . . . . . . . . . 6 2.2.3 训练模板  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2.4 DeepSeek-R1-Z\nero 的性能、自我进化过程和顿悟时刻  6\n2.3 DeepSeek-R1 ：冷启动强化学习  . . . . . . . . . . . . . . . 9  \u00002.3.1 冷启动 . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . 9  \u00002.3.2 面向推理的强化学习  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10  \u00002.3.3 拒绝\n采样与监督微调  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10  \u00002.3.4 全场景强化学习  . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . 11  \u00002.4 蒸馏：赋予小模型推理能力  . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3 实验 11\n3.1 DeepSeek-R1 评估 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2 蒸馏模型评估  . . . . . . . . . . \n. . . . . . . . . . . . . . . . . . . . . 14\n4 讨论 14\n4.1 蒸馏与强化学习  . . . . . . . . . . . . . . . . . . . . . . . . 14 4.2 未成功的尝试  . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . 15\n5 结论、局限性与未来工作  16\nA 贡献与致谢  20\n2",
      "score": 0.5345016717910767,
      "metadata": {
        "source": "DeepSeek.pdf",
        "page": "2",
        "chunk": 2,
        "total_chunks": 5,
        "page_range": "2",
        "embedding_provider": "openai",
        "embedding_model": "text-embedding-3-small",
        "embedding_timestamp": "2025-03-13T17:13:00.598215"
      }
    },
    {
      "text": "1. 引言\n近年来，大型语言模型（ LLMs ）经历了快速的迭代和进化（ Anthropic, 2024; Google, 2024; Ope\nnAI, 2024a ），逐步缩小了与人工通用智能（ AGI ）之间的差距。\n最近，后训练已成为完整训练流程中的一个重要组成部分。它已被证明可以提高推理任务的\n准确性，与社会价值观保持一致，并适应用户偏好，同时相对于预训练所需的计算资源相对较\n少。在推理能力方面， OpenAI 的o1 （OpenAI, 2024b ）系列模型首次通过增加思维链推理过程的\n长度引入了推理时扩展。这种方法在数学、编码和科学推理等各种推理任务中取得了显著改进\n。然而，有效的测试时扩展仍然是研究界的一个开放性问题。之前的一些工作探索了各种方法\n，包括基于过程的奖励模型（ Lightman 等，2023 ；Uesato 等，2022 ；Wang 等，2023 ）、强化学\n习（Kumar 等，2024 ）以及蒙特卡洛树搜索和束搜索等搜索算法（ Feng 等，2024 ；Trinh 等，202\n4 ；Xin 等，2024 ）。然而，这些方法都没有达到与 OpenAI 的o1 系列模型相媲美的通用推理性能\n。\n在本文中，我们迈出了利用纯强化学习（ RL ）提升语言模型推理能力的第一步。我们的目标\n是探索大型语言模型（ LLMs ）在没有监督数据的情况下发展推理能力的潜力，重点关注它们通\n过纯RL 过程的自我进化。具体而言，我们使用 DeepSeek-V3-Base 作为基础模型，并采用 GRPO （\nShao 等，2024 ）作为RL 框架，以提升模型在推理任务中的表现。在训练过程中， DeepSeek-R1-Z\nero 自然涌现出许多强大且有趣的推理行为。经过数千次 RL 步骤后， DeepSeek-R1-Zero 在推理基\n准测试中展现出卓越的性能。例如， AIME 2024 上的pass@1 得分从15.6% 提升至71.0% ，而在多\n数投票机制下，得分进一步提高至 86.7% ，与OpenAI-o1-0912 的性能相当。\n然而，DeepSeek-R1-Zero 遇到了诸如可读性差和语言混合等挑战。为了解决这些问题并进一\n步提升推理性能，我们引入了  DeepSeek-R1 ，它结合了少量冷启动数据和多阶段训练流程。具体\n来说，我们首先收集数千条冷启动数据来微调  DeepSeek-V3-Base 模型。随后，我们进行类似  De\nepSeek-R1-Zero 的推理导向强化学习（ RL ）。在 RL 过程接近收敛时，我们通过对  RL 检查点进\n行拒绝采样，结合来自  DeepSeek-V3 在写作、事实问答和自我认知等领域的监督数据，创建新\n的 SFT 数据，然后重新训练  DeepSeek-V3-Base 模型。使用新数据微调后，检查点会经历额外的  \nRL 过程，考虑所有场景的提示。经过这些步骤，我们获得了称为  DeepSeek-R1 的检查点，其性\n能与 OpenAI-o1-1217 相当。\n我们进一步探索了从 DeepSeek-R1 到更小密集模型的蒸馏过程。以 Qwen2.5-32B （Qwen, 2024\nb ）为基础模型，直接从 DeepSeek-R1 进行蒸馏的效果优于在其上应用强化学习。这表明，更大\n基础模型发现的推理模式对于提升推理能力至关重要。我们开源了蒸馏后的 Qwen 和Llama （Dub\ney 等，2024 ）系列。值得注意的是，我们蒸馏的 14B 模型大幅超越了当前最先进的开源 QwQ-32B\n-Preview （Qwen, 2024a ），而蒸馏的 32B 和70B 模型在密集模型的推理基准测试中创下了新纪录\n。\n3",
      "score": 0.461336612701416,
      "metadata": {
        "source": "DeepSeek.pdf",
        "page": "3",
        "chunk": 3,
        "total_chunks": 5,
        "page_range": "3",
        "embedding_provider": "openai",
        "embedding_model": "text-embedding-3-small",
        "embedding_timestamp": "2025-03-13T17:13:00.598222"
      }
    }
  ]
}